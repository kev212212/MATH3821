---
title: "MATH3821 Assignment 1"
author: "Stephen Sung"
output: pdf_document
header-includes:
  - \usepackage{enumitem}
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{graphicx}
  - \usepackage{hyperref}
  - \usepackage{bbm}
  - \usepackage{nccmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

For Simple Linear Regression model $y_i=\beta_0+\beta_1 x_i + \epsilon_i$ where $\epsilon_i \sim N(0,\sigma^2)$.

a) Let $\beta_0 = \alpha - \beta_1 \bar{x}$. Then the SLR model can be expressed as $y_i=\alpha+\beta_1(x_i-\bar{x})+\epsilon_i$.

b) $\alpha$ is the intercept of the new model?

c) To find the closed form formula of the LSE,
\begin{align}
    RSS(\beta_1) &= \sum_{i=1}^{n} [y_i-(\alpha+\beta_1(x_i-\bar{x}))]^2 \nonumber \\
    \frac{\text{d}RSS(\beta_1)}{\text{d}\alpha} &= -2 \sum_{i=1}^{n} (y_i-(\alpha+\beta_1(x_i-\bar{x})))\\
    \frac{\text{d}RSS(\beta_1)}{\text{d}\beta_1} &= -2 \sum_{i=1}^{n}   \left(y_i-(\alpha+\beta_1(x_i-\bar{x}))(x_i -\bar{x})\right)  
  \end{align}
  Let Equation $(1) = 0$
  \begin{align*}
    &-2 \sum_{i=1}^{n} (y_i-(\hat{\alpha}+\hat{\beta_1}(x_i-\bar{x}))) = 0 \\
    &\sum_{i=1}^{n} y_i - \sum_{i=1}^{n} \hat{\alpha_i} - \sum_{i=1}^{n} \hat{\beta_1}x_i + \sum_{i=1}^{n} \hat{\beta_1}\bar{x} = 0 \\
   & n \hat{\alpha_i} = \sum_{i=1}^{n} y_i - \hat{\beta_1} \sum_{i=1}^{n}x_i + n\hat{\beta_1}\bar{x} \\
    &\hat{\alpha_i} = \frac{1}{n}\sum_{i=1}^{n} y_i -\hat{\beta_1}\bar{x}+\hat{\beta_1}\bar{x} \\
    &\hat{\alpha_i} = \bar{y}
  \end{align*}
  Let Equation $(2) = 0$ 
  \begin{align*}
    &-2 \sum_{i=1}^{n} \left(y_i-(\hat{\alpha}+\hat{\beta_1}(x_i-\bar{x}))(x_i -\bar{x})\right) = 0 \\
    &\sum_{i=1}^{n} y_i(x_i-\bar{x})-\sum_{i=1}^{n}\hat{\alpha}(x_i-\bar{x}) -\sum_{i=1}^{n} \hat{\beta_1}(x_i-\bar{x})^2 = 0 \\
    &\hat{\beta_1} \sum_{i=1}^{n} (x_i-\bar{x})^2 = \sum_{i=1}^{n} (y_i-\hat{\alpha})(x_i-\bar{x}) \\ \intertext{since $\hat{\alpha} = \bar{y}$}
    &\hat{\beta_1} = \frac{\sum_{i=1}^{n} (y_i-\bar{y})(x_i-\bar{x})}{\sum_{i=1}^{n} (x_i-\bar{x})^2}
  \end{align*}

d) 
  \begin{align*}
    Var[\hat{\alpha}] &= Var\left[ \frac{1}{n} \sum_{i=1}^{n} y_i \right] \\
    &= \frac{1}{n^2} Var \left[ \sum_{i=1}^{n} y_i \right] \\ \intertext{Since all $y_i$'s are uncorrelated}
    &= \frac{1}{n^2} \sum_{i=1}^{n} Var [y_i] \\
    &= \frac{1}{n^2} \sum_{i=1}^{n} \sigma^2 \\
    &= \frac{n \sigma^2}{n^2} \\
    &= \frac{\sigma^2}{n} \\
  \end{align*}
  Therefore $Var[\hat{\alpha}] = \frac{\sigma^2}{n}$. \
  \
  We note that $\sum_{i=1}^{n} (y_i-\bar{y})(x_i-\bar{x})=\sum_{i=1}^{n} (x_i-\bar{x})y_i$, since
  \begin{align*}
    &\sum_{i=1}^{n} (y_i-\bar{y})(x_i-\bar{x}) = \sum_{i=1}^{n} y_i(x_i-\bar{x}) - \sum_{i=1}^{n} \bar{y}(x_i-\bar{x}) \\ \intertext{and we notice that}
    &\sum_{i=1}^{n} (x_i-\bar{x}) = \sum_{i=1}^{n} x_i - \frac{1}{n}n\sum_{i=1}^{n} x_i = 0
  \end{align*}
  To calculate $Var[\hat{\beta_1}]$,
  \begin{align*}
    Var[\hat{\beta_1}]&=Var\left[\frac{\sum_{i=1}^{n} (x_i-\bar{x})y_i}{\sum_{i=1}^{n} (x_i-\bar{x})^2}\right] \\
    &= \frac{1}{\left(\sum_{i=1}^{n} (x_i-\bar{x})^2\right)^2} Var\left[\sum_{i=1}^{n}(x_i-\bar{x})y_i\right] \\
    &= \frac{1}{\left(\sum_{i=1}^{n} (x_i-\bar{x})^2\right)^2}\sum_{i=1}^{n} Var\left[(x_i-\bar{x})y_i\right] \\
    &= \frac{1}{\left(\sum_{i=1}^{n} (x_i-\bar{x})^2\right)^2}\sum_{i=1}^{n} (x_i-\bar{x})^2 Var\left[y_i\right] \\
    &= \frac{1}{\left(\sum_{i=1}^{n} (x_i-\bar{x})^2\right)^2}\sum_{i=1}^{n} (x_i-\bar{x})^2 \sigma^2 \\
    &= \frac{\sigma^2}{\sum_{i=1}^{n} (x_i-\bar{x})^2}
  \end{align*}
  Let $S_{xx}=\sum_{i=1}^{n} (x_i-\bar{x})^2$ and $S_{xy}=\sum_{i=1}^{n} (x_i-\bar{x})y_i$.
  \
  To calculate $Cov[\hat{\alpha},\hat{\beta_1]}$
  \begin{align*}
    Cov[\hat{\alpha},\hat{\beta_1]} &= Cov[\bar{y},\hat{\beta_1}] \\
    &= Cov\left[\bar{y},\frac{S_{xy}}{S_{xx}}\right] \\
    &= \frac{1}{S_{xx}}Cov\left[\bar{y},S_{xy}\right] \\
    &= \frac{1}{S_{xx}}Cov\left[\frac{1}{n}\sum_{i=1}^{n}y_i,S_{xy}\right] \\
    &= \frac{1}{n S_{xx}}Cov\left[ \sum_{i=1}^{n} y_i, \sum_{j=1}^{n} (x_j-\bar{x})y_j)\right] \\
    &= \frac{1}{n S_{xx}} \sum_{i=1}^{n} \sum_{j=1}^{n} (x_j-\bar{x}) Cov[ y_i,y_j] \\ \intertext{When $i\ne j$, $Cov[y_i,y_j]=0$ since all $y_i$ are uncorrelated with each other, and $Cov[y_i,y_j]=Var[y_i]$ when $i = j$}
    &= \frac{1}{n S_{xx}} \sum_{i=j}^{n} (x_i-\bar{x}) \sigma^2 \\
    &= 0
  \end{align*}
  
e) 
```{r}
set.seed(1234567)
x = runif(1000)
eps = rnorm(1000)
y = 5 + 10*x + eps
model <- y~x
RSS <- function(b) c(-2 * sum(y - b[1] - b[2] * x), -2 * sum((y - b[1] - b[2] * x) * x))
#This function gives us the gradient of the RSS
bn <- c(0,0)
gamma <- 0.00001
kmax <- 100000

for (k in 0:kmax) {
  bnp1 <- bn - gamma*RSS(bn)
  if(sum(RSS(bn)^2) <= 0.00001){
    cat("b: ", bnp1, "-- RSS:", RSS(c(bnp1[1],bnp1[2])),"\n","Iterations:",k,"\n")
    break
    }
  bn <- bnp1
}
#This algorithm starts at b0 = 0 and b1 = 0 which is the same as alpha = 0 and b1 = 0
#I get the l2 norm of score since the RSS function gives the gradient which is the score
#thus the sum of squares of gradient (RSS) must b less than 0.00001
#alpha from minimisation
bnp1[1]+bnp1[2]*mean(x)
#Using the closed form formula in c) we find alpha
mean(y)
#Finding beta
bnp1[2]
sum(((y-mean(y))*(x-mean(x)))/(sum((x-mean(x))^2)))
#We can see that alpha has the same value as does the beta
#about 9943 iterations were required 
#Below is my working to find the gradient function for the RSS
'''
To get the gradient of the RSS we must get the first derivative of:\\
\begin{align*}
    S(\beta_1,\beta_2) &= \sum_{i=1}^n(y_i-\beta_1-\beta_2x_i)^2\\
    S'(\beta_1,\beta_2) &= (-2\sum_{i=1}^n(y_i-\beta_1-\beta_2x_i),-2\sum_{i=1}^nx_i(y_i-\beta_1-\beta_2x_i))\\
\end{align*}


f)
'''{r}
plot(x,y)
lm = lm(y~x)
abline(lm, col = "red")
abline(bnp1[1],bnp1[2], col = 'blue')
legend(0,16,legend=c("linear model", "gradient descent method"),
       col=c("red", "blue"), lty=1:2, cex=0.8)
'''

g)
'''{r}
beta = c(0,0)
imax = 100000
RSShess = function(b)cbind(c(2*length(x), 2*sum(x)),c(2*sum(x),2*sum(x^2)))
#This function gives us the hessian matrix for the RSS

for (i in 0:imax) {
  beta1 <- beta - solve(RSShess(beta))%*%RSS(beta)
  if(sum(RSS(beta)^2) <= 0.00001){
    cat("beta: ", beta1, "-- RSS:", RSS(c(beta1[1],beta1[2])),"\n","Iterations:",i,"\n")
    break
  }
  beta <- beta1
}

#alpha from NR
beta1[1]+beta1[2]*mean(x)
#Using the closed form formula in c) we find alpha
mean(y)
#Finding beta
beta1[2]
sum(((y-mean(y))*(x-mean(x)))/(sum((x-mean(x))^2)))
#We can see that values are the same 
#1 iterations was required
#It took alot less iterations than the gradient descent method
#This could be due to the fact that the Newton-raphson method
#accounts for the curvature of the loglikelihood and as such should need less iterations
#Below is my working to find the hessian matrix for the RSS
'''
To get the Hessian matrix of the RSS we need to further derive the gradient found in Question 1e with respect to $\beta_1$ and $\beta_2$.\\
\begin{align*}
    S'(\beta_1,\beta_2) &= (-2\sum_{i=1}^n(y_i-\beta_1-\beta_2x_i),-2\sum_{i=1}^nx_i(y_i-\beta_1-\beta_2x_i))\\
    \frac{\partial^2S(\beta_1,\beta_2)}{\partial\beta_1^2} &= 2\sum_{i=1}^n1\\
    &= 2n\\
    \frac{\partial^2S(\beta_1,\beta_2)}{\partial\beta_2^2} &= 2\sum_{i=1}^nx_ix_i\\
    &= 2\sum_{i=1}^nx_i^2\\
    \frac{\partial^2S(\beta_1,\beta_2)}{\partial\beta_1\partial\beta_2} &= 2\sum_{i=1}^nx_i
    &= \frac{\partial^2S(\beta_1,\beta_2)}{\partial\beta_2\partial\beta_1}\\
\end{align*}

  
## Question 2
Given \textit{n} independent binary random variables $Y_1 \cdots Y_n$ with
\begin{align*}
  P(Y_i=1)=\pi_i \text{  and  } P(Y_i=0)=1-\pi_i 
\end{align*} 
The probability function of $Y_i$ is:
\begin{align*}
  \pi_i^{Y_i}(1-\pi_i)^{1-Y_i}
\end{align*} 
where $Y_i=0$ or $1$

a) For a probability function to belong to the exponential family of distributions, it must follow the formula:

 \begin{align*}
    f(y;\theta,\phi)=K(y,\frac{p}{\phi})\exp\left(\frac{p}{\phi}\{y\theta-c(\theta)\}\right)
  \end{align*}
  For the given probability density function:
  \begin{align*}
    f(y;\pi) &= \pi_i^{y}(1-\pi_i)^{1-y} \\
    &= \exp\left(\log\pi_i^{y}(1-\pi_i)^{1-y}\right) \\
    &= \exp\left(\log\pi_i^{y}+\log(1-\pi_i)^{1-y})\right) \\
    &= \exp\left(y\log\pi_i+(1-y)\log(1-\pi_i)\right) \\
    &= \exp\left(y\log(\frac{\pi}{1-\pi})+\log(1-\pi)\right) \\
  \end{align*}
  With $p=1$ and $\phi=1$, the above equation follows the form of the exponential family of distribution where
  $K(y,\frac{p}{\phi})=1$, $\theta=\log(\frac{\pi}{1-\pi})$ and $c(\theta)=-\log(1-\pi)=-\log(1-\frac{e^{\theta}}{1+e^{\theta}})$ where $\pi=\frac{e^{\theta}}{1+e^{\theta}}$.
  
b) As seen in 2a, the naturalised parameter is $\theta=\log(\frac{\pi}{1-\pi})$
c) As seen in 2a, the cumulant generator is $c(\theta)=-\log(1-\frac{e^{\theta}}{1+e^{\theta}})$.
  Since $\text{E}[Y]=c^\prime(\theta)$,
  $c^\prime(\theta)=-(\frac{e^\theta}{1+e^\theta})=-(-\pi)=\pi$.
  Therefore, $\text{E}[Y]=\pi$.
d) Given the link function:
\begin{align*}
    g(\pi)=\log(\frac{\pi}{1-\pi})=e^{x^{T}\beta}
  \end{align*}
  it can be rearranged in terms of the probability $\pi$,
  \begin{align*}
    e^{x^{T}\beta}&=\log(\frac{\pi}{1-\pi}) \\
    e^{x^{T}\beta}-\pi e^{x^{T}\beta}&=\pi \\
    \pi &= \frac{e^{x^{T}\beta}}{1+e^{x^{T}\beta}}
  \end{align*}
e) 
```{r}
curve(exp(1+x)/(1+exp(1+x)), xlim = c(-5, 5), ylim = c(0, 1), 
      main=expression(paste("Graph of ", log(pi/1-pi),'=',x^{T},beta,'=',beta[1]+beta[2],'x'))
      )
```
It shows the log odds of the insecticide working with a given probability?

f) The following probability density function:
\begin{align*}
    f(y;\theta)=\frac{1}{\phi}\exp\left(\frac{(y-\theta)}{\phi}-\exp\left[\frac{(y-\theta)}{\phi}\right]\right)
  \end{align*}
  is NOT in the exponential family of distributions as it does not follow the form of a probability density function in the exponential family.

## Question 3

a)
```{r}
titanic <- read.table('titanic.txt', header=TRUE)
head(titanic)
```

```{r}
summary(titanic)
```


b)
```{r}
attach(titanic)
table(titanic$Sex)
```
```{r}
tapply(titanic$Survived,titanic$Sex,mean)
```

```{r}
summary(lm(titanic$Survived~titanic$Sex))
```

c)
```{r}
titanic.glm <- glm(titanic$Survived~titanic$Age,family=binomial('logit'))
summary(titanic.glm)
```

```{r}
exp(titanic.glm$coefficients[2])
```
```{r}
exp(titanic.glm$coefficients[2])
```


d)

e)

f)

h)

i)

j)
  
