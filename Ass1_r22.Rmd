---
title: "MATH3821 Assignment 1"
author: "Stephen Sung"
output: pdf_document
header-includes:
  - \usepackage{enumitem}
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{graphicx}
  - \usepackage{hyperref}
  - \usepackage{bbm}
  - \usepackage{nccmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

For Simple Linear Regression model $y_i=\beta_0+\beta_1 x_i + \epsilon_i$ where $\epsilon_i \sim N(0,\sigma^2)$.

a) Let $\beta_0 = \alpha - \beta_1 \bar{x}$. Then the SLR model can be expressed as $y_i=\alpha+\beta_1(x_i-\bar{x})+\epsilon_i$.

b) $\alpha$ is the intercept of the new model?

c) To find the closed form formula of the LSE,
\begin{align}
    RSS(\beta_1) &= \sum_{i=1}^{n} [y_i-(\alpha+\beta_1(x_i-\bar{x}))]^2 \nonumber \\
    \frac{\text{d}RSS(\beta_1)}{\text{d}\alpha} &= -2 \sum_{i=1}^{n} (y_i-(\alpha+\beta_1(x_i-\bar{x})))\\
    \frac{\text{d}RSS(\beta_1)}{\text{d}\beta_1} &= -2 \sum_{i=1}^{n}   \left(y_i-(\alpha+\beta_1(x_i-\bar{x}))(x_i -\bar{x})\right)  
  \end{align}
  Let Equation $(1) = 0$
  \begin{align*}
    &-2 \sum_{i=1}^{n} (y_i-(\hat{\alpha}+\hat{\beta_1}(x_i-\bar{x}))) = 0 \\
    &\sum_{i=1}^{n} y_i - \sum_{i=1}^{n} \hat{\alpha_i} - \sum_{i=1}^{n} \hat{\beta_1}x_i + \sum_{i=1}^{n} \hat{\beta_1}\bar{x} = 0 \\
   & n \hat{\alpha_i} = \sum_{i=1}^{n} y_i - \hat{\beta_1} \sum_{i=1}^{n}x_i + n\hat{\beta_1}\bar{x} \\
    &\hat{\alpha_i} = \frac{1x}{n}\sum_{i=1}^{n} y_i -\hat{\beta_1}\bar{x}+\hat{\beta_1}\bar{x} \\
    &\hat{\alpha_i} = \bar{y}
  \end{align*}
  Let Equation $(2) = 0$ 
  \begin{align*}
    &-2 \sum_{i=1}^{n} \left(y_i-(\hat{\alpha}+\hat{\beta_1}(x_i-\bar{x}))(x_i -\bar{x})\right) = 0 \\
    &\sum_{i=1}^{n} y_i(x_i-\bar{x})-\sum_{i=1}^{n}\hat{\alpha}(x_i-\bar{x}) -\sum_{i=1}^{n} \hat{\beta_1}(x_i-\bar{x})^2 = 0 \\
    &\hat{\beta_1} \sum_{i=1}^{n} (x_i-\bar{x})^2 = \sum_{i=1}^{n} (y_i-\hat{\alpha})(x_i-\bar{x}) \\ \intertext{since $\hat{\alpha} = \bar{y}$}
    &\hat{\beta_1} = \frac{\sum_{i=1}^{n} (y_i-\bar{y})(x_i-\bar{x})}{\sum_{i=1}^{n} (x_i-\bar{x})^2}
  \end{align*}

d) 
  \begin{align*}
    Var[\hat{\alpha}] &= Var\left[ \frac{1}{n} \sum_{i=1}^{n} y_i \right] \\
    &= \frac{1}{n^2} Var \left[ \sum_{i=1}^{n} y_i \right] \\ \intertext{Since all $y_i$'s are uncorrelated}
    &= \frac{1}{n^2} \sum_{i=1}^{n} Var [y_i] \\
    &= \frac{1}{n^2} \sum_{i=1}^{n} \sigma^2 \\
    &= \frac{n \sigma^2}{n^2} \\
    &= \frac{\sigma^2}{n} \\
  \end{align*}
  Therefore $Var[\hat{\alpha}] = \frac{\sigma^2}{n}$. \
  \
  We note that $\sum_{i=1}^{n} (y_i-\bar{y})(x_i-\bar{x})=\sum_{i=1}^{n} (x_i-\bar{x})y_i$, since
  \begin{align*}
    &\sum_{i=1}^{n} (y_i-\bar{y})(x_i-\bar{x}) = \sum_{i=1}^{n} y_i(x_i-\bar{x}) - \sum_{i=1}^{n} \bar{y}(x_i-\bar{x}) \\ \intertext{and we notice that}
    &\sum_{i=1}^{n} (x_i-\bar{x}) = \sum_{i=1}^{n} x_i - \frac{1}{n}n\sum_{i=1}^{n} x_i = 0
  \end{align*}
  To calculate $Var[\hat{\beta_1}]$,
  \begin{align*}
    Var[\hat{\beta_1}]&=Var\left[\frac{\sum_{i=1}^{n} (x_i-\bar{x})y_i}{\sum_{i=1}^{n} (x_i-\bar{x})^2}\right] \\
    &= \frac{1}{\left(\sum_{i=1}^{n} (x_i-\bar{x})^2\right)^2} Var\left[\sum_{i=1}^{n}(x_i-\bar{x})y_i\right] \\
    &= \frac{1}{\left(\sum_{i=1}^{n} (x_i-\bar{x})^2\right)^2}\sum_{i=1}^{n} Var\left[(x_i-\bar{x})y_i\right] \\
    &= \frac{1}{\left(\sum_{i=1}^{n} (x_i-\bar{x})^2\right)^2}\sum_{i=1}^{n} (x_i-\bar{x}) Var\left[y_i\right] \\
    &= \frac{1}{\left(\sum_{i=1}^{n} (x_i-\bar{x})^2\right)^2}\sum_{i=1}^{n} (x_i-\bar{x}) \sigma^2 \\
    &= \frac{\sigma^2}{\sum_{i=1}^{n} (x_i-\bar{x})^2}
  \end{align*}
  Let $S_{xx}=\sum_{i=1}^{n} (x_i-\bar{x})^2$ and $S_{xy}=\sum_{i=1}^{n} (x_i-\bar{x})y_i$.
  \
  To calculate $Cov[\hat{\alpha},\hat{\beta_1]}$
  \begin{align*}
    Cov[\hat{\alpha},\hat{\beta_1]} &= Cov[\bar{y},\hat{\beta_1}] \\
    &= Cov\left[\bar{y},\frac{S_{xy}}{S_{xx}}\right] \\
    &= \frac{1}{S_{xx}}Cov\left[\bar{y},S_{xy}\right] \\
    &= \frac{1}{S_{xx}}Cov\left[\frac{1}{n}\sum_{i=1}^{n}y,S_{xy}\right] \\
    &= \frac{1}{n S_{xx}}Cov\left[ \sum_{i=1}^{n} y, \sum_{j=1}^{n} (x_j-\bar{x})y_j)\right] \\
    &= \frac{1}{n S_{xx}} \sum_{i=1}^{n} \sum_{j=1}^{n} (x_j-\bar{x}) Cov[ y_i,y_j] \\
    &= \frac{1}{n S_{xx}} \sum_{i=j}^{n} (x_i-\bar{x}) \sigma^2 \\
    &= 0
  \end{align*}
  
e) 
```{r}
norm_vec <- function(x) sqrt(sum(x^2))
#function to fins l2 norm
set.seed(1234567)
x = runif(1000)
eps = rnorm(1000)
y = 5 + 10*x + eps
model <- y~x
RSS <- function(b) c(-2 * sum(y - b[1] - b[2] * x), -2 * sum((y - b[1] - b[2] * x) * x))
bn <- c(0,0)
gamma <- 0.00001
kmax <- 1000000

for (k in 0:kmax) {
  bnp1 <- bn - gamma*RSS(bn)
  y_new = bnp1[1]+bnp1[2]*x
  score = c(sum(y_new),-500+0.5*sum(y_new^2))
  #Since it comes from a standard normal
  #Is this the correct score function because it never reaches below 0.00001
  if(norm_vec(score) < 0.00001){
    cat("b: ", bnp1, "-- RSS:", RSS(c(bnp1[1],bnp1[2])),"\n","Iterations:",k,"\n")
    break
    }
  bn <- bnp1
}
#alpha from minimisation
bnp1[1]+bnp1[2]*mean(x)
#Using the closed form formula in c) we find alpha
mean(y)
#Finding beta
bnp1[2]
sum(((y-mean(y))*(x-mean(x)))/(sum(x-mean(x))))
#We can see that alpha has the same value hoever for beta we achieve a very different value
#1000000 iterations were required
'''
f)
```{r}
plot(x,y)
lm = lm(y~x)
abline(lm, col = "red")
```
g)
  
## Question 2
Given \textit{n} independent binary random variables $Y_1 \cdots Y_n$ with
\begin{align*}
  P(Y_i=1)=\pi_i \text{  and  } P(Y_i=0)=1-\pi_i 
\end{align*} 
The probability function of $Y_i$ is:
\begin{align*}
  \pi_i^{Y_i}(1-\pi_i)^{1-Y_i}
\end{align*} 
where $Y_i=0$ or $1$

a) For a probability function to belong to the exponential family of distributions, it must follow the formula:

 \begin{align*}
    f(y;\theta,\phi)=K(y,\frac{p}{\phi})\exp\left(\frac{p}{\phi}\{y\theta-c(\theta)\}\right)
  \end{align*}
  For the given probability density function:
  \begin{align*}
    f(y;\pi) &= \pi_i^{y}(1-\pi_i)^{1-y} \\
    &= \exp\left(\log\pi_i^{y}(1-\pi_i)^{1-y}\right) \\
    &= \exp\left(\log\pi_i^{y}+\log(1-\pi_i)^{1-y})\right) \\
    &= \exp\left(y\log\pi_i+(1-y)\log(1-\pi_i)\right) \\
    &= \exp\left(y\log(\frac{\pi}{1-\pi})+\log(1-\pi)\right) \\
  \end{align*}
  With $p=1$ and $\phi=1$, the above equation follows the form of the exponential family of distribution where
  $K(y,\frac{p}{\phi})=1$, $\theta=\log(\frac{\pi}{1-\pi})$ and $c(\theta)=-\log(1-\pi)=-\log(1-\frac{e^{\theta}}{1+e^{\theta}})$ where $\pi=\frac{e^{\theta}}{1+e^{\theta}}$.
  
b) As seen in 2a, the naturalised parameter is $\theta=\log(\frac{\pi}{1-\pi})$
c) As seen in 2a, the cumulant generator is $c(\theta)=-\log(1-\frac{e^{\theta}}{1+e^{\theta}})$.
  Since $\text{E}[Y]=c^\prime(\theta)$,
  $c^\prime(\theta)=-(\frac{e^\theta}{1+e^\theta})=-(-\pi)=\pi$.
  Therefore, $\text{E}[Y]=\pi$.
d) Given the link function:
\begin{align*}
    g(\pi)=\log(\frac{\pi}{1-\pi})=e^{x^{T}\beta}
  \end{align*}
  it can be rearranged in terms of the probability $\pi$,
  \begin{align*}
    e^{x^{T}\beta}&=\log(\frac{\pi}{1-\pi}) \\
    e^{x^{T}\beta}-\pi e^{x^{T}\beta}&=\pi \\
    \pi &= \frac{e^{x^{T}\beta}}{1+e^{x^{T}\beta}}
  \end{align*}
e) 
```{r}
curve(exp(1+x)/(1+exp(1+x)), xlim = c(-5, 5), ylim = c(0, 1), main=expression(paste("Graph of", log(pi/1-pi))))
```
It shows the log odds of the insecticide working with a given probability?

f) The following probability density function:
\begin{align*}
    f(y;\theta)=\frac{1}{\phi}\exp\left(\frac{(y-\theta)}{\phi}-\exp\left[\frac{(y-\theta)}{\phi}\right]\right)
  \end{align*}
  is NOT in the exponential family of distributions as it does not follow the form of a probability density function in the exponential family.

## Question 3

a)
```{r}
titanic <- read.table('titanic.txt', header=TRUE)
head(titanic)
```

```{r}
summary(titanic)
```


b)
```{r}
attach(titanic)
table(titanic$Sex)
```
```{r}
tapply(titanic$Survived,titanic$Sex,mean)
```

```{r}
summary(lm(titanic$Survived~titanic$Sex))
```

c)
```{r}
titanic.glm <- glm(titanic$Survived~titanic$Age,family=binomial('logit'))
summary(titanic.glm)
1```


d)

e)

f)

h)

i)

j)
  
